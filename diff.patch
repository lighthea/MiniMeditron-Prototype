diff --git a/__pycache__/bastifonctions.cpython-311.pyc b/__pycache__/bastifonctions.cpython-311.pyc
deleted file mode 100644
index 4fcd921..0000000
Binary files a/__pycache__/bastifonctions.cpython-311.pyc and /dev/null differ
diff --git a/conf/config_finetune_m2_reinforce.json b/conf/config_finetune_m2_reinforce.json
new file mode 100644
index 0000000..16671a3
--- /dev/null
+++ b/conf/config_finetune_m2_reinforce.json
@@ -0,0 +1,53 @@
+{
+    "general_settings" : {
+        "process_file": "data/structure/pipelines/pipeline_2.json",
+        "base_model_id": "HuggingFaceH4/zephyr-7b-beta",
+        "task": "qa",
+        "use_flash_attn": false
+    },
+
+    "general_folders" : {
+        "guidelines_folder": "data/knowledge_database/guidelines/structured_guidelines",
+        "train_folder": "data/knowledge_database/generated_patients/structured_patients",
+        "base_folder": "data/structure/guidelines"
+    },
+
+    "model_folders" : {
+        "tokenized_data_path": "data/exports/dataset/model2_qa/processed",
+        "output_dir": "data/exports/finetuned_models/model2_qa",
+        "save_path": "data/exports/finetuned_models/model2_qa/checkpoints"
+    },
+
+    "dataset_generation" : {
+        "force_retokenize": false,
+        "n_context_guidelines": 3,
+        "with_token": false,
+        "with_output": false,
+        "test_size": 0.1
+    },
+
+    "model_parameters" : {
+        "num_train_epochs": 2,
+        "per_device_train_batch_size": 1,
+        "eval_steps" : 50,
+        "save_steps": 50,
+        "logging_steps": 25,
+        "learning_rate": 5e-5,
+        "max_steps": 2000,
+        "neftune_noise_alpha": 5,
+        "eval_accumulation_steps": 1,
+        "gradient_checkpointing": true,
+        "gradient_accumulation_steps": 1,
+        "gradient_checkpointing_kwargs" :  {"use_reentrant": false}
+    },
+
+    "wandb_parameters" : {
+        "start_from_checkpoint": true,
+        "baseline_name" : "generalist-model",
+        "run_name": "model2-qa-ppo-0",
+        "wandb_key": "27a738bc8c5d8ad7ce8429de93f4f4ddc637b567",
+        "wandb_project": "minimed-finetune-proto0",
+        "wandb_folder": "data/exports/wandb",
+	"reinit_weights": true
+    }
+}
diff --git a/data/exports/dataset/model2_qa/processed/dataset_dict.json b/data/exports/dataset/model2_qa/processed/dataset_dict.json
new file mode 100644
index 0000000..31145ea
--- /dev/null
+++ b/data/exports/dataset/model2_qa/processed/dataset_dict.json
@@ -0,0 +1 @@
+{"splits": ["train", "test"]}
\ No newline at end of file
diff --git a/data/exports/dataset/model2_qa/processed/test/data-00000-of-00001.arrow b/data/exports/dataset/model2_qa/processed/test/data-00000-of-00001.arrow
new file mode 100644
index 0000000..7950280
Binary files /dev/null and b/data/exports/dataset/model2_qa/processed/test/data-00000-of-00001.arrow differ
diff --git a/data/exports/dataset/model2_qa/processed/test/dataset_info.json b/data/exports/dataset/model2_qa/processed/test/dataset_info.json
new file mode 100644
index 0000000..4827e2e
--- /dev/null
+++ b/data/exports/dataset/model2_qa/processed/test/dataset_info.json
@@ -0,0 +1,12 @@
+{
+  "citation": "",
+  "description": "",
+  "features": {
+    "text": {
+      "dtype": "string",
+      "_type": "Value"
+    }
+  },
+  "homepage": "",
+  "license": ""
+}
\ No newline at end of file
diff --git a/data/exports/dataset/model2_qa/processed/test/state.json b/data/exports/dataset/model2_qa/processed/test/state.json
new file mode 100644
index 0000000..950126c
--- /dev/null
+++ b/data/exports/dataset/model2_qa/processed/test/state.json
@@ -0,0 +1,13 @@
+{
+  "_data_files": [
+    {
+      "filename": "data-00000-of-00001.arrow"
+    }
+  ],
+  "_fingerprint": "2b10d8ed31dffdb6",
+  "_format_columns": null,
+  "_format_kwargs": {},
+  "_format_type": null,
+  "_output_all_columns": false,
+  "_split": null
+}
\ No newline at end of file
diff --git a/data/exports/dataset/model2_qa/processed/train/data-00000-of-00001.arrow b/data/exports/dataset/model2_qa/processed/train/data-00000-of-00001.arrow
new file mode 100644
index 0000000..5ad7fc1
Binary files /dev/null and b/data/exports/dataset/model2_qa/processed/train/data-00000-of-00001.arrow differ
diff --git a/data/exports/dataset/model2_qa/processed/train/dataset_info.json b/data/exports/dataset/model2_qa/processed/train/dataset_info.json
new file mode 100644
index 0000000..4827e2e
--- /dev/null
+++ b/data/exports/dataset/model2_qa/processed/train/dataset_info.json
@@ -0,0 +1,12 @@
+{
+  "citation": "",
+  "description": "",
+  "features": {
+    "text": {
+      "dtype": "string",
+      "_type": "Value"
+    }
+  },
+  "homepage": "",
+  "license": ""
+}
\ No newline at end of file
diff --git a/data/exports/dataset/model2_qa/processed/train/state.json b/data/exports/dataset/model2_qa/processed/train/state.json
new file mode 100644
index 0000000..dfa23a6
--- /dev/null
+++ b/data/exports/dataset/model2_qa/processed/train/state.json
@@ -0,0 +1,13 @@
+{
+  "_data_files": [
+    {
+      "filename": "data-00000-of-00001.arrow"
+    }
+  ],
+  "_fingerprint": "a21556673fe0c516",
+  "_format_columns": null,
+  "_format_kwargs": {},
+  "_format_type": null,
+  "_output_all_columns": false,
+  "_split": null
+}
\ No newline at end of file
diff --git a/lib/__pycache__/__init__.cpython-310.pyc b/lib/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index 2688a70..0000000
Binary files a/lib/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/lib/__pycache__/block.cpython-310.pyc b/lib/__pycache__/block.cpython-310.pyc
deleted file mode 100644
index fd2ca54..0000000
Binary files a/lib/__pycache__/block.cpython-310.pyc and /dev/null differ
diff --git a/lib/__pycache__/env.cpython-310.pyc b/lib/__pycache__/env.cpython-310.pyc
deleted file mode 100644
index f550fb0..0000000
Binary files a/lib/__pycache__/env.cpython-310.pyc and /dev/null differ
diff --git a/lib/__pycache__/metrics.cpython-310.pyc b/lib/__pycache__/metrics.cpython-310.pyc
deleted file mode 100644
index a020c6e..0000000
Binary files a/lib/__pycache__/metrics.cpython-310.pyc and /dev/null differ
diff --git a/lib/__pycache__/pipeline.cpython-310.pyc b/lib/__pycache__/pipeline.cpython-310.pyc
deleted file mode 100644
index 187295e..0000000
Binary files a/lib/__pycache__/pipeline.cpython-310.pyc and /dev/null differ
diff --git a/lib/__pycache__/training.cpython-310.pyc b/lib/__pycache__/training.cpython-310.pyc
deleted file mode 100644
index 37c6e87..0000000
Binary files a/lib/__pycache__/training.cpython-310.pyc and /dev/null differ
diff --git a/scripts/__pycache__/preprocess_for_mistral.cpython-311.pyc b/scripts/__pycache__/preprocess_for_mistral.cpython-311.pyc
deleted file mode 100644
index 3c267b1..0000000
Binary files a/scripts/__pycache__/preprocess_for_mistral.cpython-311.pyc and /dev/null differ
diff --git a/scripts/install.sh b/scripts/install.sh
index c110e11..eee18b9 100755
--- a/scripts/install.sh
+++ b/scripts/install.sh
@@ -40,3 +40,10 @@ echo "Running the fine_tune.py file"
 if [ -n "$1" ]; then
     exec accelerate launch --config_file conf/accelerate_config.yaml src/train.py "$1"
 fi
+
+echo "Install ssh server"
+sudo apt-get update
+sudo apt install -y openssh-server
+mkdir ~/.ssh
+echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCuzeqyo4+/BSm10YrhWNz1OkTljrJ7xeLh3t8EFIqxH8qeIdT+EhE9MNurYYwzM6b+rD8qow7ijFdf61PsqV2Z0liKXb5A6kXAaIEChfHzbc2IvX3Tgwdfzk6fMsnzL5B3IQ1bDGckOrBOH90zTp7DEDOSPnHnW+XAGL2sCMUDxyb77WtGZAJtgAV8V0WPntNDYr8GsDgCFoBRhgWIGrxj8WCEfpme871rQ4AacAbsf2iEM0lkd0UAOxHqMS1G3onNOESsmlFsyrJwDCCHWgqjo0Z0Rlv5ejGeufagTtVOGA81A8jMYGh2iGoL06IMFNu5RQfP5CLVxsLKCU2OUFsytOVVgEeFXGE15B4UpMAsbovWJemCd0yKBnV7v4DZsL30cRLSdtDkU2P/TXtuAQctJ7CJEoXuTkxz9X3s8mRSweD5mpxRwwp2JQFJ5136oPWxiGzcYb7qwxHOTwqlRFA/V2osfrdUZKlSaFgj0OKaiD/wuHlK0IvWCWTNeuePF4s= antoine@antoine-MACHC-WAX9" >> ~/.ssh/authorized_keys
+service ssh start
diff --git a/src/ppo_train.py b/src/ppo_train.py
new file mode 100644
index 0000000..bbfa142
--- /dev/null
+++ b/src/ppo_train.py
@@ -0,0 +1,134 @@
+import os
+import sys
+import torch
+from random import random
+
+from torch.optim import Adam
+from transformers import AutoTokenizer, AutoModelForCausalLM, AutoTokenizer
+from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, create_reference_model, set_seed
+from trl.core import LengthSampler
+
+current_dir = os.path.dirname(os.path.realpath(__file__))
+sys.path.append(os.path.join(current_dir, '..'))
+
+from lib.training import load_config, init_configs
+from lib.dataset import load_dataset
+from tqdm import tqdm
+
+# Create the ppo config
+config_path = sys.argv[1]
+config = load_config(config_path)
+
+# Retrieve the model name
+model_name = config["general_settings"]["base_model_id"]
+
+# Build the ppo config
+ppo_config = PPOConfig(
+    model_name=model_name,
+    learning_rate=config["model_parameters"]["learning_rate"],
+    ppo_epochs=100,
+    mini_batch_size=1,
+    batch_size=4,
+    optimize_device_cache=True,
+    gradient_accumulation_steps=1
+)
+
+# GPT-2 / GPT-J tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.
+# only for this model.
+tokenizer = AutoTokenizer.from_pretrained(ppo_config.model_name)
+tokenizer.pad_token = tokenizer.eos_token
+
+# Build the dataset and tokenize the sample(s)
+dataset = load_dataset(config, tokenizer)
+# dataset = dataset.rename_column("text", "query")
+
+input_size = LengthSampler(2, 512)
+
+def tokenize(sample):
+    prompt = sample["text"]
+
+    sample["input_ids"] = tokenizer.encode(prompt)[: input_size()]
+    sample["query"] = tokenizer.decode(sample["input_ids"])
+    return sample
+
+dataset = dataset.map(tokenize, batched=False)
+dataset.remove_columns('text')
+dataset.set_format('torch')
+
+dataset = dataset["train"]
+
+# Set the seed for initializing value head for deterministic eval
+set_seed(42)
+
+# Now let's build the model, the reference model, and the tokenizer. We first
+# load the model in bfloat16 to save memory using `transformers`
+model = AutoModelForCausalLM.from_pretrained(ppo_config.model_name, torch_dtype=torch.bfloat16)
+
+# And then we pass the loaded model to `AutoModelForCausalLMWithValueHead`
+model = AutoModelForCausalLMWithValueHead.from_pretrained(model)
+
+# We create a reference model by sharing 20 layers
+ref_model = create_reference_model(model, num_shared_layers=20)
+
+# We make sure to use the `Adam` optimizer on the model parameters that requires gradients
+optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=ppo_config.learning_rate)
+
+# What the f*ck is a collator
+def collator(data):
+    return dict((key, [d[key] for d in data]) for key in data[0])
+
+# We then build the PPO Trainer, passing the model, the reference model, the tokenizer
+ppo_trainer = PPOTrainer(
+    config=ppo_config,
+    model=model,
+    ref_model=ref_model,
+    tokenizer=tokenizer,
+    data_collator=collator,
+    dataset=dataset,
+    optimizer=optimizer,
+)
+
+# We then build the reward pipeline, we will use the random model to compute the reward
+def compute_rewards(texts):
+    return torch.rand(len(texts))
+
+# We then define the arguments to pass to the `generate` function. These arguments
+# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
+# the `generate` function of the trained model.
+generation_kwargs = {
+    "min_length": -1,
+    "top_k": 0.0,
+    "top_p": 1.0,
+    "do_sample": True,
+    "pad_token_id": tokenizer.eos_token_id,
+}
+output_length_sampler = LengthSampler(4, 128)
+
+for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
+    query_tensors = batch["input_ids"]
+
+    # Get response from the policy model
+    response_tensors = []
+    for query in query_tensors:
+        gen_len = output_length_sampler()
+        generation_kwargs["max_new_tokens"] = gen_len
+        response = ppo_trainer.generate(query, **generation_kwargs)
+        response_tensors.append(response.squeeze()[-gen_len:])
+    batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]
+
+    print('PBUzoyefbrosuygrouyvg')
+
+    # Compute rewards using our own reward function
+    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
+    pipe_outputs = compute_rewards(texts)
+    rewards = [torch.tensor(output) for output in pipe_outputs]
+
+    # Run a PPO step
+    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
+    ppo_trainer.log_stats(stats, batch, rewards)
+    print(stats)
+
+    # Save model every 100 epochs
+    # if epoch % 100 == 0:
+    #     if ppo_trainer.accelerator.is_main_process:
+    #         ppo_trainer.save_pretrained(model_save_path)
diff --git a/src/reinforcement_training_block.py b/src/reinforcement_training_block.py
index a7cbeb9..f01dc45 100644
--- a/src/reinforcement_training_block.py
+++ b/src/reinforcement_training_block.py
@@ -1,86 +1,133 @@
 import os
 import sys
-
 import torch
+from random import random
+
+from torch.optim import Adam
+from accelerate import Accelerator
+from peft import LoraConfig
 from datasets import DatasetDict
-from transformers import AutoTokenizer
+from transformers import AutoTokenizer, AutoModelForCausalLM
 from trl import PPOConfig, AutoModelForCausalLMWithValueHead, PPOTrainer
+from trl.core import LengthSampler
 
 current_dir = os.path.dirname(os.path.realpath(__file__))
 sys.path.append(os.path.join(current_dir, '..'))
 
 from lib.training import load_config, init_wandb_project, init_configs
+from lib.dataset import load_dataset
 from tqdm import tqdm
 
+# Data collators are objects that will form a batch by using a list of dataset elements 
+# as input. These elements are of the same type as the elements of `train_dataset` or 
+# `eval_dataset`.
+def collator(data):
+    return dict((key, [d[key] for d in data]) for key in data[0])
 
 def main():
     # Load configuration
     conf_file = sys.argv[1]
     config = load_config(conf_file)
-    model_name = config["model_parameters"]["base_model_id"]
+    
+    model_name = config["general_settings"]["base_model_id"]
     ppo_config = PPOConfig(
         model_name=model_name,
-        learning_rate=1.41e-5,
-        log_with="wandb"
+        learning_rate=config["model_parameters"]["learning_rate"],
+        gradient_accumulation_steps=config["model_parameters"]["gradient_accumulation_steps"],
+        optimize_device_cache=True,
+        log_with="wandb",
+        batch_size=config["model_parameters"]["per_device_train_batch_size"],
+        task_name=config["wandb_parameters"]["run_name"],
+        tracker_project_name=config["wandb_parameters"]["wandb_project"],
     )
 
     # Initialize the wandb project
     init_wandb_project(config)
 
-    bnb_config, ia3_conf = init_configs(config)
+    # Initialize the accelerator and quantization configs
+    # Not used in practice (I have no clue on how to make it work with PPO trainer)
+    bnb_config, ia3_config = init_configs(config)
 
-    model = AutoModelForCausalLMWithValueHead.from_pretrained(ppo_config.model_name)
-    tokenizer = AutoTokenizer.from_pretrained(ppo_config.model_name)
+    # Now let's build the model, the reference model, and the tokenizer. We first
+    # load the model in bfloat16 to save memory using `transformers`
+    # model = AutoModelForCausalLM.from_pretrained(ppo_config.model_name, torch_dtype=torch.bfloat16)
+
+    # And then we pass the loaded model to `AutoModelForCausalLMWithValueHead`
+    model = AutoModelForCausalLMWithValueHead.from_pretrained(
+        ppo_config.model_name,
+        peft_config=ia3_config,
+        device_map={ "": Accelerator().local_process_index }
+    )
+
+    tokenizer = AutoTokenizer.from_pretrained(ppo_config.model_name) #, add_eos_token=True)
     tokenizer.pad_token = tokenizer.eos_token
-    tokenizer.padding_side = 'right'
+    # tokenizer.padding_side = 'right'
 
     # Define the reward function as a random number between 0 and 1
-    def reward_model(x):
-        return torch.rand(10)
+    def compute_rewards(texts):
+        return torch.rand(len(texts))
+
+    dataset: DatasetDict = load_dataset(config, tokenizer)
+    # dataset = dataset.rename_column("text", "query")
 
-    train_dataset: DatasetDict = load_dataset(config, tokenizer, None, with_token=False, with_output=False)
-    train_dataset = train_dataset.rename_column("text", "query")
+    input_size = LengthSampler(2, 512)
 
     def tokenize(sample):
-        sample["input_ids"] = tokenizer.encode(sample["query"], padding="max_length", max_length=4096)
+        prompt = sample["text"]
+
+        sample["input_ids"] = tokenizer.encode(prompt)[: input_size()]
+        sample["query"] = tokenizer.decode(sample["input_ids"])
         return sample
 
-    train_dataset = train_dataset.map(tokenize, batched=False)
+    dataset = dataset.map(tokenize, batched=False)
+    dataset.remove_columns("text")
+    dataset.set_format('torch')
+
+    # We make sure to use the `Adam` optimizer on the model parameters that requires gradients
+    optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=ppo_config.learning_rate)
+
     ppo_trainer = PPOTrainer(
-        model=model,
         config=ppo_config,
-        dataset=train_dataset["train"],
+        model=model,
+        dataset=dataset["train"],
+        data_collator=collator,
+        optimizer=optimizer,
         tokenizer=tokenizer,
     )
 
+    max_new_tokens = 128
+
     generation_kwargs = {
         "min_length": -1,
         "top_k": 0.0,
         "top_p": 1.0,
         "do_sample": True,
         "pad_token_id": tokenizer.eos_token_id,
-        "max_new_tokens": 1024
+        "max_new_tokens": max_new_tokens
     }
 
-    tokenizer.padding_side = 'left'
+    # tokenizer.padding_side = 'left'
 
     for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
         query_tensors = batch["input_ids"]
 
         # Get response from SFTModel
-        response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
+        response_tensors = []
+        for query in query_tensors:
+            response = ppo_trainer.generate(query, **generation_kwargs)
+            response_tensors.append(response.squeeze()[-max_new_tokens:])
         batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]
 
         # Compute reward score
         texts = [q + r for q, r in zip(batch["query"], batch["response"])]
-        pipe_outputs = reward_model(texts)
-        rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
+        pipe_outputs = compute_rewards(texts)
+        rewards = [torch.tensor(output) for output in pipe_outputs]
 
         # Run PPO step
-        stats = ppo_trainer.step(query_tensors, [response_tensors], rewards)
+        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
         ppo_trainer.log_stats(stats, batch, rewards)
-        print(stats)
-
+        # print(stats)
 
 if __name__ == "__main__":
     main()
+
