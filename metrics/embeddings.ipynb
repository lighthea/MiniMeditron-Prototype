{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that you have the correct embeddings and create the structure around it.\n",
    "\n",
    "Need to check what is the structure of the embeddings with Nanogpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def tensor_distance(tensor1, tensor2, distance_type=\"L2\"):\n",
    "    \"\"\"\n",
    "    Compute the L2 (Euclidean) distance between two tensors of the same shape.\n",
    "    \n",
    "    Args:\n",
    "    - tensor1 (torch.Tensor): The first tensor.\n",
    "    - tensor2 (torch.Tensor): The second tensor.\n",
    "    - distance_type (str): The type of distance to compute. Currently only\n",
    "    \n",
    "    Returns:\n",
    "    - float: The distance / similarity between the two tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    if tensor1.shape != tensor2.shape:\n",
    "        raise ValueError(\"Both tensors must have the same shape.\")\n",
    "\n",
    "    if distance_type == \"L2\":\n",
    "        distance = torch.norm(tensor1 - tensor2)\n",
    "    elif distance_type == \"Manhattan\":\n",
    "        distance = torch.sum(torch.abs(tensor1 - tensor2))\n",
    "    elif distance_type == \"Cosine\":\n",
    "        similarity = F.cosine_similarity(tensor1, tensor2)\n",
    "        distance = 1 - similarity\n",
    "    elif distance_type == \"Minkowski\":\n",
    "        distance = torch.norm(tensor1 - tensor2, p=3)\n",
    "    \n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 distance between tensor_a and tensor_b: tensor(2.3165)\n",
      "Manhattan distance between tensor_a and tensor_b: tensor(12.9438)\n",
      "Cosine distance between tensor_a and tensor_b: tensor([[0.0208, 0.0554, 0.0341, 0.0544, 0.2788, 0.4471, 0.2120, 0.0249, 0.3111,\n",
      "         0.1120, 0.0279, 0.2833, 0.4034, 0.1088, 0.0700]])\n",
      "Minkowski distance between tensor_a and tensor_b: tensor(1.3923)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "tensor_a = torch.rand((1, 3, 15))\n",
    "tensor_b = torch.rand((1, 3, 15)) \n",
    "\n",
    "distance_types = [\"L2\", \"Manhattan\", \"Cosine\", \"Minkowski\"]\n",
    "\n",
    "for dist in distance_types:\n",
    "    print(f\"{dist} distance between tensor_a and tensor_b:\", tensor_distance(tensor_a, tensor_b, dist))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from model import GPT, GPTConfig\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "res = generator(\"dommage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/home/etien/Documents/EPFLcourses/MA3/Meditron/nanoGPT/model.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.69M\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m     27\u001b[0m \u001b[39m# Usage\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m model \u001b[39m=\u001b[39m load_model(\u001b[39m\"\u001b[39;49m\u001b[39m/home/etien/Documents/EPFLcourses/MA3/Meditron/nanoGPT/checkpoint/checkpoint.ckpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(checkpoint_path, config)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m checkpoint_path \u001b[39mand\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(checkpoint_path):\n\u001b[1;32m     22\u001b[0m     checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(checkpoint_path)\n\u001b[0;32m---> 23\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model'"
     ]
    }
   ],
   "source": [
    "def load_model(checkpoint_path=None, config=None):\n",
    "    \"\"\"\n",
    "    Load the GPT model. If a checkpoint path is provided, load weights from the checkpoint.\n",
    "    \n",
    "    Args:\n",
    "    - checkpoint_path (str, optional): Path to the model checkpoint.\n",
    "    - config (GPTConfig, optional): Configuration for the model. If not provided, uses the default configuration.\n",
    "\n",
    "    Returns:\n",
    "    - model (GPT): The instantiated model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the provided config or create a default one\n",
    "    if config is None:\n",
    "        config = GPTConfig()\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = GPT(config)\n",
    "\n",
    "    # If a saved checkpoint is provided, load it\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage\n",
    "model = load_model(\"/home/etien/Documents/EPFLcourses/MA3/Meditron/nanoGPT/checkpoint/checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/etien/Documents/EPFLcourses/MA3/Meditron/nanoGPT/checkpoint/checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mgenerate(\u001b[39m\"\u001b[39m\u001b[39mBonjour\u001b[39m\u001b[39m\"\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, do_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, temperature\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.generate(\"Bonjour\", max_length=100, do_sample=True, temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
