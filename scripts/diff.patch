diff --git a/.gitignore b/.gitignore
index ffccf14..85a7668 100644
--- a/.gitignore
+++ b/.gitignore
@@ -4,5 +4,6 @@
 /Guidelines
 /data/knowledge_database/
 **/__pycache__/**
+__pycache__
 wandb/
 notes.md
\ No newline at end of file
diff --git a/.vscode/launch.json b/.vscode/launch.json
new file mode 100644
index 0000000..eadd624
--- /dev/null
+++ b/.vscode/launch.json
@@ -0,0 +1,17 @@
+{
+    // Use IntelliSense to learn about possible attributes.
+    // Hover to view descriptions of existing attributes.
+    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
+    "version": "0.2.0",
+    "configurations": [
+        {
+            "name": "Python: Current File",
+            "type": "python",
+            "request": "launch",
+            "program": "${file}",
+            "console": "integratedTerminal",
+            "justMyCode": true,
+            "args" : "conf/config_ppo_contrastive_m2.json"
+        }
+    ]
+}
\ No newline at end of file
diff --git a/conf/config_finetune_m2.json b/conf/config_finetune_m2.json
index 815a6f9..164dd35 100644
--- a/conf/config_finetune_m2.json
+++ b/conf/config_finetune_m2.json
@@ -38,15 +38,15 @@
         "neftune_noise_alpha": 5,
         "eval_accumulation_steps": 1,
         "gradient_checkpointing": true,
-        "gradient_accumulation_steps": 2,
+        "gradient_accumulation_steps": 4,
         "gradient_checkpointing_kwargs" :  {"use_reentrant": false}
     },
 
     "wandb_parameters" : {
         "start_from_checkpoint": true,
-        "baseline_name" : "generalist-model",
+        "baseline_name" : "model2-qa-reinit",
         "run_name": "model2-qa-contrastive",
-        "reinit_weights" : true,
+        "reinit_weights" : false,
         "wandb_key": "ENCRYPT{r7mfHokpVPge9wKJnIVRtQ==;NutfIIDjyMpweJ3cgI+YHVrvyyfu0Dqqyn/Ujpp0drVPJuHBzlpdlQ==;RucHJE+LLM5U/dLde7eAOA==}",
         "wandb_project": "minimed-finetune-proto0",
         "wandb_folder": "data/exports/wandb"
diff --git a/data/exports/dataset/model2_qa/processed/dataset_dict.json b/data/exports/dataset/model2_qa/processed/dataset_dict.json
new file mode 100644
index 0000000..31145ea
--- /dev/null
+++ b/data/exports/dataset/model2_qa/processed/dataset_dict.json
@@ -0,0 +1 @@
+{"splits": ["train", "test"]}
\ No newline at end of file
diff --git a/data/exports/dataset/model2_qa/processed/test/cache-4cdd040896ecdd92.arrow b/data/exports/dataset/model2_qa/processed/test/cache-4cdd040896ecdd92.arrow
new file mode 100644
index 0000000..59cbadb
Binary files /dev/null and b/data/exports/dataset/model2_qa/processed/test/cache-4cdd040896ecdd92.arrow differ
diff --git a/data/exports/dataset/model2_qa/processed/test/cache-5d191c00c095483d.arrow b/data/exports/dataset/model2_qa/processed/test/cache-5d191c00c095483d.arrow
new file mode 100644
index 0000000..4a3649f
Binary files /dev/null and b/data/exports/dataset/model2_qa/processed/test/cache-5d191c00c095483d.arrow differ
diff --git a/data/exports/dataset/model2_qa/processed/test/cache-7f812e5e80eac4c7.arrow b/data/exports/dataset/model2_qa/processed/test/cache-7f812e5e80eac4c7.arrow
new file mode 100644
index 0000000..522dcbd
Binary files /dev/null and b/data/exports/dataset/model2_qa/processed/test/cache-7f812e5e80eac4c7.arrow differ
diff --git a/data/exports/dataset/model2_qa/processed/test/cache-89cf2af0906b132d.arrow b/data/exports/dataset/model2_qa/processed/test/cache-89cf2af0906b132d.arrow
new file mode 100644
index 0000000..a1bb3c2
Binary files /dev/null and b/data/exports/dataset/model2_qa/processed/test/cache-89cf2af0906b132d.arrow differ
diff --git a/data/exports/dataset/model2_qa/processed/test/cache-9744c7b1c4a7651e.arrow b/data/exports/dataset/model2_qa/processed/test/cache-9744c7b1c4a7651e.arrow
new file mode 100644
index 0000000..3196963
Binary files /dev/null and b/data/exports/dataset/model2_qa/processed/test/cache-9744c7b1c4a7651e.arrow differ
diff --git a/data/exports/dataset/model2_qa/processed/test/data-00000-of-00001.arrow b/data/exports/dataset/model2_qa/processed/test/data-00000-of-00001.arrow
new file mode 100644
index 0000000..b3928dc
Binary files /dev/null and b/data/exports/dataset/model2_qa/processed/test/data-00000-of-00001.arrow differ
diff --git a/data/exports/dataset/model2_qa/processed/test/dataset_info.json b/data/exports/dataset/model2_qa/processed/test/dataset_info.json
new file mode 100644
index 0000000..4827e2e
--- /dev/null
+++ b/data/exports/dataset/model2_qa/processed/test/dataset_info.json
@@ -0,0 +1,12 @@
+{
+  "citation": "",
+  "description": "",
+  "features": {
+    "text": {
+      "dtype": "string",
+      "_type": "Value"
+    }
+  },
+  "homepage": "",
+  "license": ""
+}
\ No newline at end of file
diff --git a/data/exports/dataset/model2_qa/processed/test/state.json b/data/exports/dataset/model2_qa/processed/test/state.json
new file mode 100644
index 0000000..46e01b8
--- /dev/null
+++ b/data/exports/dataset/model2_qa/processed/test/state.json
@@ -0,0 +1,13 @@
+{
+  "_data_files": [
+    {
+      "filename": "data-00000-of-00001.arrow"
+    }
+  ],
+  "_fingerprint": "1a1507cd113597c1",
+  "_format_columns": null,
+  "_format_kwargs": {},
+  "_format_type": null,
+  "_output_all_columns": false,
+  "_split": null
+}
\ No newline at end of file
diff --git a/data/exports/dataset/model2_qa/processed/train/cache-4f699b2aac995540.arrow b/data/exports/dataset/model2_qa/processed/train/cache-4f699b2aac995540.arrow
new file mode 100644
index 0000000..550238d
Binary files /dev/null and b/data/exports/dataset/model2_qa/processed/train/cache-4f699b2aac995540.arrow differ
diff --git a/data/exports/dataset/model2_qa/processed/train/cache-a616e8dcd100ee46.arrow b/data/exports/dataset/model2_qa/processed/train/cache-a616e8dcd100ee46.arrow
new file mode 100644
index 0000000..997de8e
Binary files /dev/null and b/data/exports/dataset/model2_qa/processed/train/cache-a616e8dcd100ee46.arrow differ
diff --git a/data/exports/dataset/model2_qa/processed/train/cache-b03e12cfe2e9b1bb.arrow b/data/exports/dataset/model2_qa/processed/train/cache-b03e12cfe2e9b1bb.arrow
new file mode 100644
index 0000000..05b1bac
Binary files /dev/null and b/data/exports/dataset/model2_qa/processed/train/cache-b03e12cfe2e9b1bb.arrow differ
diff --git a/data/exports/dataset/model2_qa/processed/train/data-00000-of-00001.arrow b/data/exports/dataset/model2_qa/processed/train/data-00000-of-00001.arrow
new file mode 100644
index 0000000..8be09bc
Binary files /dev/null and b/data/exports/dataset/model2_qa/processed/train/data-00000-of-00001.arrow differ
diff --git a/data/exports/dataset/model2_qa/processed/train/dataset_info.json b/data/exports/dataset/model2_qa/processed/train/dataset_info.json
new file mode 100644
index 0000000..4827e2e
--- /dev/null
+++ b/data/exports/dataset/model2_qa/processed/train/dataset_info.json
@@ -0,0 +1,12 @@
+{
+  "citation": "",
+  "description": "",
+  "features": {
+    "text": {
+      "dtype": "string",
+      "_type": "Value"
+    }
+  },
+  "homepage": "",
+  "license": ""
+}
\ No newline at end of file
diff --git a/data/exports/dataset/model2_qa/processed/train/state.json b/data/exports/dataset/model2_qa/processed/train/state.json
new file mode 100644
index 0000000..7d0957c
--- /dev/null
+++ b/data/exports/dataset/model2_qa/processed/train/state.json
@@ -0,0 +1,13 @@
+{
+  "_data_files": [
+    {
+      "filename": "data-00000-of-00001.arrow"
+    }
+  ],
+  "_fingerprint": "992f7107c1828d2c",
+  "_format_columns": null,
+  "_format_kwargs": {},
+  "_format_type": null,
+  "_output_all_columns": false,
+  "_split": null
+}
\ No newline at end of file
diff --git a/data/exports/dataset/model2_qa/processed/train/tmp3z7z541e b/data/exports/dataset/model2_qa/processed/train/tmp3z7z541e
new file mode 100644
index 0000000..9e0dc8f
Binary files /dev/null and b/data/exports/dataset/model2_qa/processed/train/tmp3z7z541e differ
diff --git a/lib/__pycache__/__init__.cpython-310.pyc b/lib/__pycache__/__init__.cpython-310.pyc
index 2688a70..d11236c 100644
Binary files a/lib/__pycache__/__init__.cpython-310.pyc and b/lib/__pycache__/__init__.cpython-310.pyc differ
diff --git a/lib/__pycache__/training.cpython-310.pyc b/lib/__pycache__/training.cpython-310.pyc
index 37c6e87..d82fbb7 100644
Binary files a/lib/__pycache__/training.cpython-310.pyc and b/lib/__pycache__/training.cpython-310.pyc differ
diff --git a/run_reinforcement.sh b/run_reinforcement.sh
old mode 100644
new mode 100755
diff --git a/scripts/diff.patch b/scripts/diff.patch
index 37f1d1f..9a1027d 100644
--- a/scripts/diff.patch
+++ b/scripts/diff.patch
@@ -1,15 +0,0 @@
-diff --git a/.gitignore b/.gitignore
-index ffccf14..54001a7 100644
---- a/.gitignore
-+++ b/.gitignore
-@@ -5,4 +5,4 @@
- /data/knowledge_database/
- **/__pycache__/**
- wandb/
--notes.md
-\ No newline at end of file
-+notes.md
-diff --git a/__pycache__/bastifonctions.cpython-311.pyc b/__pycache__/bastifonctions.cpython-311.pyc
-deleted file mode 100644
-index 4fcd921..0000000
-Binary files a/__pycache__/bastifonctions.cpython-311.pyc and /dev/null differ
diff --git a/scripts/local_commit.sh b/scripts/local_commit.sh
index 9b1c4e8..37e7f56 100755
--- a/scripts/local_commit.sh
+++ b/scripts/local_commit.sh
@@ -1,4 +1,4 @@
-scp -P 2255 root@127.0.0.1:MiniMeditron-Prototype/scripts/diff.patch diff.patch
+scp -P 2244 root@127.0.0.1:MiniMeditron-Prototype/scripts/diff.patch diff.patch
 git apply diff.patch
 git add -A
 git commit -m "$1"
diff --git a/src/reinforcement_training_block.py b/src/reinforcement_training_block.py
index db55d10..d3e03c6 100644
--- a/src/reinforcement_training_block.py
+++ b/src/reinforcement_training_block.py
@@ -5,6 +5,8 @@ import torch
 from datasets import DatasetDict
 from transformers import AutoTokenizer
 from trl import PPOConfig, AutoModelForCausalLMWithValueHead, PPOTrainer
+from accelerate import Accelerator
+
 
 from secure_env import *
 
@@ -17,48 +19,59 @@ from tqdm import tqdm
 from lib.dataset import load_dataset
 
 
+def collator(data):
+    return dict((key, [d[key] for d in data]) for key in data[0])
+
 def main():
     # Load configuration
     conf_file = sys.argv[1]
     
     config = load_config(conf_file)
-    config = secure_config(config)
+    # config = secure_config(config)
 
     model_name = config["general_settings"]["base_model_id"]
     ppo_config = PPOConfig(
         model_name=model_name,
         learning_rate=1.41e-5,
-        log_with="wandb"
+        batch_size=256,
+        optimize_device_cache=True,
+        # gradient_accumulation_steps=config["model_parameters"]["gradient_accumulation_steps"]
+        # log_with="wandb"
     )
 
     # Initialize the wandb project
-    init_wandb_project(config)
+    # init_wandb_project(config)
 
     bnb_config, ia3_conf = init_configs(config)
 
-    model = AutoModelForCausalLMWithValueHead.from_pretrained(ppo_config.model_name)
+    model = AutoModelForCausalLMWithValueHead.from_pretrained(ppo_config.model_name, device_map={ "": Accelerator().local_process_index })
     tokenizer = AutoTokenizer.from_pretrained(ppo_config.model_name)
     tokenizer.pad_token = tokenizer.eos_token
-    tokenizer.padding_side = 'right'
+    tokenizer.padding_side = 'left'
 
     # Define the reward function as a random number between 0 and 1
-    def reward_model(x):
-        print(x)
+    def reward_model(queries, responses):
+        # print(x)
         return torch.rand(10)
 
     train_dataset: DatasetDict = load_dataset(config, tokenizer)
     train_dataset = train_dataset.rename_column("text", "query")
 
     def tokenize(sample):
-        sample["input_ids"] = tokenizer.encode(sample["query"], padding="max_length", max_length=4096)
+        sample["input_ids"] = tokenizer.encode(sample["query"], padding="max_length", max_length=2048)
+        sample["query"] = tokenizer.decode(sample["input_ids"])
+
         return sample
 
     train_dataset = train_dataset.map(tokenize, batched=False)
+    train_dataset.set_format("torch")
+
     ppo_trainer = PPOTrainer(
         model=model,
         config=ppo_config,
         dataset=train_dataset["train"],
         tokenizer=tokenizer,
+        data_collator=collator
     )
 
     generation_kwargs = {
@@ -67,10 +80,9 @@ def main():
         "top_p": 1.0,
         "do_sample": True,
         "pad_token_id": tokenizer.eos_token_id,
-        "max_new_tokens": 64
+        "max_new_tokens": 512
     }
 
-    tokenizer.padding_side = 'left'
 
     for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
         print(f">>> Epoch {epoch} <<<", end='\n')
@@ -86,8 +98,8 @@ def main():
         print("[x] - Tokenized", end='\n')
 
         # Compute reward score
-        texts = [q + r for q, r in zip(batch["query"], batch["response"])]
-        pipe_outputs = reward_model(texts)
+        # texts = [q + r for q, r in zip(batch["query"], batch["response"])]
+        pipe_outputs = reward_model(batch["query"], batch["response"])
         print("[x] - Computed Reward", end='\n')
         
         rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
