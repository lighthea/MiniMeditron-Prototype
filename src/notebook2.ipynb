{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, exists\n",
    "from os import listdir\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKIDATA_ENDPOINT_URL = \"https://query.wikidata.org/sparql\"\n",
    "DISEASE_OUT_CACHED_FILE = join(\"..\", \"query-output-cached-{}.json\")\n",
    "STRUCTURED_GUIDELINES_FOLDER_PATH = join(\"..\", \"data\", \"knowledge_database\", \"guidelines\", \"structured_guidelines\")\n",
    "SEPARATOR = \"<|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "def cache_result(query):\n",
    "    H = hashlib.sha256(query.encode(\"utf-8\")).hexdigest()[:8]\n",
    "    cache_file = DISEASE_OUT_CACHED_FILE.format(H)\n",
    "    if not exists(cache_file):\n",
    "        print(\" [x] Running query: {}\".format(H))\n",
    "        contents = get_results(WIKIDATA_ENDPOINT_URL, query)\n",
    "        \n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(contents, f)\n",
    "    else:\n",
    "        print(\" [+] Using cached query: {}\".format(H))\n",
    "        with open(cache_file, 'r') as f:\n",
    "            contents = json.load(f)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [+] Using cached query: e55817f7\n",
      " [+] Using cached query: b3fd0a42\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT DISTINCT\n",
    "    ?item\n",
    "    ?itemLabel\n",
    "    (GROUP_CONCAT(DISTINCT ?subclass_of_label; SEPARATOR=\"{SEPARATOR}\") AS ?subclass_of)\n",
    "    (GROUP_CONCAT(DISTINCT ?study_by_label; SEPARATOR=\"{SEPARATOR}\") AS ?study_by)\n",
    "    (GROUP_CONCAT(DISTINCT ?health_speciality_label; SEPARATOR=\"{SEPARATOR}\") AS ?health_speciality) \n",
    "    (GROUP_CONCAT(DISTINCT ?symptoms_and_signs_label; SEPARATOR=\"{SEPARATOR}\") AS ?symptoms_and_signs) # ?study_by\n",
    "WHERE {\n",
    "    ?item wdt:P31/wdt:P279* wd:Q112193867.\n",
    "    OPTIONAL { ?item wdt:P279 ?subclass_of. }\n",
    "    OPTIONAL { ?item wdt:P2579 ?study_by. }\n",
    "    OPTIONAL { ?item wdt:P1995 ?health_speciality. }\n",
    "    OPTIONAL { ?item wdt:P780 ?symptoms_and_signs. }\n",
    "    SERVICE wikibase:label {\n",
    "        bd:serviceParam wikibase:language \"en\".\n",
    "        ?subclass_of rdfs:label ?subclass_of_label .\n",
    "        ?study_by rdfs:label ?study_by_label .\n",
    "        ?health_speciality rdfs:label ?health_speciality_label .\n",
    "        ?symptoms_and_signs rdfs:label ?symptoms_and_signs_label .\n",
    "        ?item rdfs:label ?itemLabel .\n",
    "    }\n",
    "}\n",
    "GROUP BY ?item ?itemLabel\n",
    "\"\"\".replace(\"{SEPARATOR}\", SEPARATOR)\n",
    "\n",
    "query2 = \"\"\"\n",
    "SELECT DISTINCT\n",
    "  ?item\n",
    "  (GROUP_CONCAT(DISTINCT ?alt_label; SEPARATOR=\"{SEPARATOR}\") AS ?alt_labels2)\n",
    "WHERE {\n",
    "    ?item wdt:P31/wdt:P279* wd:Q112193867.\n",
    "    OPTIONAL { ?item skos:altLabel ?alt_label . FILTER (lang(?alt_label) = \"en\") }\n",
    "    SERVICE wikibase:label {\n",
    "      bd:serviceParam wikibase:language \"en\".\n",
    "    }\n",
    "}\n",
    "GROUP BY ?item\n",
    "\"\"\".replace(\"{SEPARATOR}\", SEPARATOR)\n",
    "\n",
    "contents = cache_result(query)\n",
    "alt_contents = cache_result(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "alt_table = {}\n",
    "\n",
    "def map_list(it, fn):\n",
    "    return [i for i in map(fn, it) if i is not None]\n",
    "\n",
    "def map_if(val, fn):\n",
    "    if val is None:\n",
    "        return None\n",
    "    return fn(val)\n",
    "\n",
    "def retrieve(object_, key, default):\n",
    "    keys = key.split('/')\n",
    "    for key in keys:\n",
    "        object_ = object_.get(key, None)\n",
    "        if object_ is None:\n",
    "            return default\n",
    "    return object_\n",
    "\n",
    "for alt in alt_contents[\"results\"][\"bindings\"]:\n",
    "    id = map_if(retrieve(alt, \"item/value\", None), lambda x: x.split('/')[-1])\n",
    "    alt = map_if(retrieve(alt, \"alt_labels2/value\", None), lambda x: x.split(SEPARATOR))\n",
    "    alt_table[id] = alt\n",
    "\n",
    "\n",
    "for content in contents[\"results\"][\"bindings\"]:\n",
    "    label_en = retrieve(content, 'itemLabel/value', None)\n",
    "    if label_en is None:\n",
    "        continue\n",
    "\n",
    "    id = map_if(retrieve(content, \"item/value\", None), lambda x: x.split('/')[-1])\n",
    "    elem = {\n",
    "        \"id\": id,\n",
    "        \"name\": label_en,\n",
    "        \"alt\": alt_table[id] if id in alt_table else []\n",
    "    }\n",
    "\n",
    "    for key in [\"subclass_of\", \"study_by\", \"health_speciality\", \"symptoms_and_signs\"]:\n",
    "        elem[key] = map_if(retrieve(content, key + \"/value\", None), lambda x: x.split(SEPARATOR) if len(x) > 0 else [])\n",
    "\n",
    "    dataset[id] = elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', ''}\n",
    "\n",
    "def partial_lowercase(word):\n",
    "    if all(x in string.ascii_uppercase + string.digits + '-' for x in word):\n",
    "        return word\n",
    "    return word.lower()\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    sentence = unidecode(sentence)\n",
    "    f1 = [x.group().lower() for x in re.finditer(r'[a-zA-Z0-9\\-]+', sentence)]\n",
    "    f2 = [x for x in f1 if not x in STOP_WORDS]\n",
    "    f3 = [x for x in f2 if not all(y in string.digits for y in x)]\n",
    "    return f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_index_tags = []\n",
    "dataset_index_ids = []\n",
    "\n",
    "for elem in dataset.values():\n",
    "    id = elem['id']\n",
    "    names = elem['alt'] + [elem['name']]\n",
    "    for name in names:\n",
    "        name = tokenizer(name)\n",
    "\n",
    "        if name == []:\n",
    "            continue\n",
    "\n",
    "        dataset_index_tags.append(name)\n",
    "        dataset_index_ids.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def search(query):\n",
    "    xs = tokenizer(query)\n",
    "    r = difflib.get_close_matches(xs, dataset_index_tags, n=5, cutoff=0.1)\n",
    "\n",
    "    # The version bellow ensure that keys stays order\n",
    "    return list(dict.fromkeys([dataset_index_ids[dataset_index_tags.index(r)] for r in r]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1315/1315 [02:23<00:00,  9.16it/s]\n"
     ]
    }
   ],
   "source": [
    "guidelines = []\n",
    "\n",
    "for file in listdir(STRUCTURED_GUIDELINES_FOLDER_PATH):\n",
    "    path = join(STRUCTURED_GUIDELINES_FOLDER_PATH, file)\n",
    "    if path.endswith('.jsonl'):\n",
    "        with open(path, 'r') as f:\n",
    "            guidelines += list(map(json.loads, f.readlines()))\n",
    "\n",
    "matched_guidelines = []\n",
    "for guideline in tqdm(guidelines):\n",
    "    matched_guidelines.append(\n",
    "        dict(list(guideline.items()) + [(\"matched\", search(guideline['label']))])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched proportion: 4.49% (59 elements)\n"
     ]
    }
   ],
   "source": [
    "A = len([x for x in matched_guidelines if x[\"matched\"] == []])\n",
    "print(\"Unmatched proportion: {:.2f}% ({} elements)\".format(A / len(matched_guidelines) * 100, A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
